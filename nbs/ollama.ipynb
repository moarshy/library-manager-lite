{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ollama pull nomic-embed-text, gemma3:4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/arshath/play/naveen/library-manager-lite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nest_asyncio import apply\n",
    "from pypdf import PdfReader\n",
    "import numpy as np\n",
    "from adapters.llm_provider import get_llm_provider, LitellmProvider\n",
    "\n",
    "apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"/Users/arshath/play/aflow.pdf\"\n",
    "reader = PdfReader(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = get_llm_provider(\"ollama-embed\")\n",
    "chat_model_ollama = get_llm_provider(\"ollama-chat\")\n",
    "chat_model_anthropic = get_llm_provider(\"recommender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 2000, overlap: int = 200) -> list[str]:\n",
    "    \"\"\"Splits text into overlapping chunks.\"\"\"\n",
    "    chunks = []\n",
    "    if not text:\n",
    "        return chunks\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def get_embeddings(texts: list[str], model: LitellmProvider) -> np.ndarray:\n",
    "    \"\"\"Generates embeddings for a list of texts and returns them as a numpy array.\"\"\"\n",
    "    if not model:\n",
    "        raise ValueError(\"Embedding model is not initialized.\")\n",
    "    embeddings = model.embed_text(texts)\n",
    "    if embeddings is None:\n",
    "        raise RuntimeError(\"Failed to get embeddings.\")\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def find_top_k_chunks(\n",
    "    query: str,\n",
    "    text_chunks: list[str],\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    embedding_model: LitellmProvider,\n",
    "    top_k: int = 3\n",
    ") -> list[str]:\n",
    "    \"\"\"Finds top_k most similar chunks to a query using cosine similarity.\"\"\"\n",
    "    if not embedding_model:\n",
    "        raise ValueError(\"Embedding model is not initialized.\")\n",
    "    \n",
    "    query_embedding = embedding_model.embed_text(query)\n",
    "    if query_embedding is None:\n",
    "        raise RuntimeError(\"Failed to get query embedding.\")\n",
    "    \n",
    "    query_embedding = np.array(query_embedding)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    dot_products = np.dot(chunk_embeddings, query_embedding)\n",
    "    chunk_norms = np.linalg.norm(chunk_embeddings, axis=1)\n",
    "    query_norm = np.linalg.norm(query_embedding)\n",
    "    \n",
    "    if query_norm == 0 or np.any(chunk_norms == 0):\n",
    "        # Avoid division by zero\n",
    "        return []\n",
    "        \n",
    "    similarities = dot_products / (chunk_norms * query_norm)\n",
    "    \n",
    "    # Get top_k indices in descending order\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    return [text_chunks[i] for i in top_k_indices]\n",
    "\n",
    "def simple_rag_retrieval(\n",
    "    query: str, \n",
    "    text_chunks: list[str], \n",
    "    chunk_embeddings: np.ndarray, \n",
    "    embedding_model: LitellmProvider, \n",
    "    top_k: int = 3\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Performs the retrieval step of a simple RAG pipeline.\n",
    "    \"\"\"\n",
    "    if not embedding_model:\n",
    "        print(\"Embedding model not available.\")\n",
    "        return []\n",
    "\n",
    "    return find_top_k_chunks(\n",
    "        query=query, \n",
    "        text_chunks=text_chunks,\n",
    "        chunk_embeddings=chunk_embeddings, \n",
    "        embedding_model=embedding_model, \n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "def question_answer(query: str, retrieved_chunks: list[str], chat_model: LitellmProvider) -> str:\n",
    "    \"\"\"\n",
    "    Generates an answer to a query based on retrieved text chunks using a chat model.\n",
    "    \"\"\"\n",
    "\n",
    "    if not chat_model:\n",
    "        return \"Chat model is not available.\"\n",
    "\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Please answer the user's query based on the provided context.\n",
    "    If the answer is not available in the context, please state that you cannot answer the question from the given context.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    user_content = f\"Query: {query}\"\n",
    "\n",
    "    try:\n",
    "        answer = chat_model.completion(prompt=prompt, user_content=user_content, output_format='text')\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error during chat completion: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_text(text)\n",
    "embeddings = get_embeddings(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the main idea of the document?\"\n",
    "top_k = 3\n",
    "\n",
    "retrieved_chunks = simple_rag_retrieval(\n",
    "    query=query, \n",
    "    text_chunks=chunks,  \n",
    "    chunk_embeddings=embeddings,\n",
    "    embedding_model=embedding_model,\n",
    "    top_k=top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = question_answer(query, retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
